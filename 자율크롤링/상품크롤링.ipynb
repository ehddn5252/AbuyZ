{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 상품 정보 크롤링  \n",
    "* urllib 라이브러리 사용해 파일 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✏️ 상품 페이지 목록 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 읽어들이기\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# 파일 주소 크롤링\n",
    "def bs_parsing(url):\n",
    "  response = requests.get(root_url + url)\n",
    "  soup = bs(response.text, \"html.parser\")\n",
    "  elements = soup.select(\"table#flisttable td > a\")\n",
    "  \n",
    "  # 주소 담을 리스트\n",
    "  page_list = []\n",
    "  \n",
    "  # 상세 페이지 주소 리스트화\n",
    "  for (idx, el) in enumerate(elements, 1):\n",
    "    page_list.append(root_url + el.attrs[\"href\"].replace(\"%2F\", \"/\"))\n",
    "    \n",
    "  return page_list\n",
    "\n",
    "# 리스트 값을 txt 파일에 한 줄씩 저장\n",
    "def save_text_file(page_list):\n",
    "  f = open(\"C:/Users/sodud/study/ssafy_spec_pjt/subtitle/data/page_url_list.txt\", \"w\", encoding=\"UTF8\")\n",
    "  for page in page_list:\n",
    "    f.write(page)\n",
    "    f.write(\"\\n\")\n",
    "  f.close\n",
    "  (\"[SUCCESS] 자막 페이지 목록 저장\")\n",
    "\n",
    "# 자막 사이트\n",
    "root_url = \"https://kitsunekko.net\"\n",
    "\n",
    "# 일본어 자막 목록 페이지\n",
    "page_url = \"/dirlist.php?dir=subtitles/japanese/\"\n",
    "\n",
    "# 자막 페이지 목록 리스트로 저장\n",
    "page_list = bs_parsing(page_url)\n",
    "\n",
    "# 리스트를 txt 파일로 저장\n",
    "save_text_file(page_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✏️ 상품 정보 엑셀로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "\n",
    "wb = Workbook(write_only=True)\n",
    "ws = wb.create_sheet('상품정보')\n",
    "# 컬럼 명\n",
    "ws.append([ '이름', '가격', '설명 이미지', '원산지', '상품 상태', '생산자&수입자', '할인율', '리뷰 평점', '배송료', '상품사진1','상품사진2','상품사진3'])\n",
    "\n",
    "# 임시 url\n",
    "temp_url= \"https://www.lotteon.com/p/product/LO1987818462?areaCode=AD_CPC&entryPoint=ad&dp_infw_cd=CASFC01200000&clickId=C33295679634\"\n",
    "\n",
    "\n",
    "# 웹 서버에 요청하기\n",
    "# res = requests.get(temp_url)\n",
    "# res.raise_for_status()\n",
    "\n",
    "\n",
    "webpage = requests.get(\"https://www.lotteon.com/p/product/LO1987818462?areaCode=AD_CPC&entryPoint=ad&dp_infw_cd=CASFC01200000&clickId=C33295679634\")\n",
    "soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "temp = soup.select(\"div.productName\")\n",
    "temp2 = soup.select('.productName')\n",
    "print(temp)\n",
    "print(temp2)\n",
    "temp3 = soup.find('div',class_ = 'productName')\n",
    "print(temp3)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#     product_name = web_html.select_one('.productName').get_text()\n",
    "#     upload_data = web_html.select('.top_cell__3DnEV em')[2].get_text()\n",
    "#     average_grade = web_html.find(class_=\"top_grade__3jjdl\").get_text()[2:5]\n",
    "#     low_price = web_html.select('.productList_price__2FKhU em')[0].get_text()\n",
    "#     high_price = web_html.select('.productList_price__2FKhU em')[-1].get_text()\n",
    "#     five_point = web_html.select('.filter_top_list__3rOdK em')[1].get_text()[1:-1]\n",
    "#     four_point = web_html.select('.filter_top_list__3rOdK em')[2].get_text()[1:-1]\n",
    "#     three_point = web_html.select('.filter_top_list__3rOdK em')[3].get_text()[1:-1]\n",
    "#     two_point = web_html.select('.filter_top_list__3rOdK em')[4].get_text()[1:-1]\n",
    "#     one_point = web_html.select('.filter_top_list__3rOdK em')[5].get_text()[1:-1]\n",
    "    \n",
    "# print(web_html.find_all('.productName'))\n",
    "\n",
    "# soup 객체 만들기\n",
    "# soup = BeautifulSoup(temp_url, \"html.parser\")\n",
    "# temp = soup.select(\"div\", class_=\".productName\")\n",
    "# print(temp)\n",
    "\n",
    "\n",
    "\n",
    "# product_name = web_html.select_one('h2').children.get_text()\n",
    "# product_price = web_html.select_one('h2').children.get_text()\n",
    "# des_picture = web_html.select_one('h2').children.get_text()\n",
    "# origin = web_html.select_one('h2').children.get_text()\n",
    "# status = web_html.select_one('h2').children.get_text()\n",
    "# producer = web_html.select_one('h2').children.get_text()\n",
    "# discount_rate = web_html.select_one('h2').children.get_text()\n",
    "# review_rating = web_html.select_one('h2').children.get_text()\n",
    "# delivery_fee = web_html.select_one('h2').children.get_text()\n",
    "# product_thumbnail_1 = web_html.select_one('h2').children.get_text()\n",
    "# product_thumbnail_2 = web_html.select_one('h2').children.get_text()\n",
    "# product_thumbnail_3 = web_html.select_one('h2').children.get_text()\n",
    "    \n",
    "    \n",
    "\n",
    "# ws.append([product_name, product_price, des_picture, origin, status, producer, discount_rate, review_rating, delivery_fee, product_thumbnail_1, product_thumbnail_2, product_thumbnail_3 ])\n",
    "\n",
    "\n",
    "# wb.save('상품정보.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1위: 참교육-103화\n",
      "2위: 퀘스트지상주의-54화 가족 건들면 뒤진다고\n",
      "3위: 윈드브레이커-4부 - 34화 마음의 평온\n",
      "4위: 뷰티풀 군바리-352화_소수로서 첫 시위 (2)\n",
      "5위: 신화급 귀속 아이템을 손에 넣었다-22화\n",
      "6위: 팔이피플-55화 - 계모임의 향방\n",
      "7위: 장씨세가 호위무사-181화\n",
      "8위: 버림받은 왕녀의 은밀한 침실-18화. 한밤의 습격자\n",
      "9위: 싸움독학-151화 : 보미 보고 싶다\n",
      "10위: 리턴 투 플레이어-100화. 약속 (2)\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 준비하기\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url =\"https://comic.naver.com/webtoon/weekday\"\n",
    "\n",
    "# 엑셀 파일로 저장하기\n",
    "filename = \"네이버 웹툰 인기 순위.csv\"\n",
    "f = open(filename, \"w\", encoding=\"utf-8-sig\", newline=\"\")\n",
    "writer = csv.writer(f)\n",
    "\n",
    "columns_name = [\"순위\", \"웹툰명\"] # 컬럼 속성명 만들기\n",
    "\n",
    "writer.writerow(columns_name)\n",
    "\n",
    "# 웹 서버에 요청하기\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "\n",
    "# soup 객체 만들기\n",
    "soup = BeautifulSoup(res.text, \"lxml\")\n",
    "cartoonsBox = soup.find('ol', attrs={\"class\": \"asideBoxRank\"}) # 전체 영역에서 'a' 태그를 찾지 않고 인기 급상승 영역으로 범위 제한\n",
    "cartoons = cartoonsBox.find_all('a') # 인기 급상승 영역에서 'a'태그 모두 찾아 변수 cartoons에 할당\n",
    "\n",
    "i = 1\n",
    "\n",
    "# 반복문으로 제목 가져오기(터미널 창 출력 및 엑셀 저장)\n",
    "for cartoon in cartoons: \n",
    "  title = cartoon.get(\"title\") \n",
    "  print(f\"{str(i)}위: {title}\")\n",
    "  data = [str(i), title]\n",
    "  writer.writerow(data)\n",
    "  i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "eebaf1173d8d9c3c4ee9a7b8bb1432a7f576348d6cb7a26bc263375fbc310797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
